This is some sample of the codes I used for my PhD Dissertation: Optimal Codebook and its applications. These codes tests my hypotheses on the MNIST and CIFAR datasets.
Abstract: The process of encoding and decoding an original dataset is fundamental to a variety of applications, including lossy compression in information theory and autoencoders in machine learning. In this thesis, we propose a framework to study this process through the concepts of codepage and codebook. Given a data set N and a space of codes F, an ϵ-codepage A is a subset of F that can encode and then decode N with an acceptable error of ϵ. A codebook, in turn, is a function that assigns each acceptable error ϵ a corresponding codepage in F. By extending the concept of the external covering number for a totally bounded set to an arbitrary set, we establish the existence criteria for both codepages and codebooks in more general settings. We also explore the problem of optimality, specifically the task of identifying codepages or codebooks that minimize certain cost functions. We define the cost of a codebook as the integral of a cost function applied to the individual codepages. Two types of cost functions are proposed for the codepages. In the first type, the cost of each codepage is determined by the cost of the codes it contains, which is represented by a Borel measure on the space of codes. For the case where N is totally bounded, we provide criteria for the existence of an optimal codebook. Assuming further that N belongs to a Heine-Borel space, we derive a formula for the minimum cost. In the second type, the cost of each codepage takes into account the combined costs of encoding, decoding, and the error in the encoding-decoding process. We define a topology on the set of codepages based on the Hausdorff distance and use it to establish criteria for the existence of an optimal codebook under this cost function. Throughout the thesis, we illustrate these concepts by applying them to the MNIST dataset.
